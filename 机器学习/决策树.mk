决策树是基于树的结构来进行决策的。
决策树的每个判定问题都是对某个属性的测试，每个测试的结果或是导出最终的结论或是导出进一步的判定问题。

一般的决策树包含一个根节点、若干个内结点和若干个叶子结点，其中叶子结点对应决策结果，内结点对应属性的判定问题。

决策树学习的基本算法：
![决策树学习的基本算法](./figure/决策树的基本算法.jpg)

##### 划分选择
信息熵：测量样本D的纯度。假设当前样本集合D中第k类样本所占的比例为$ p_k$,其中$ k = (1, 2, 3, \dots, |y|)$，则信息熵定义为：
$$ Ent(D) = \sum_{k = 1}^{|y|}{p_klog_2p_k}$$

信息增益：
+ 假设离散属性a有V个可能取值$ {a^1, a^2, \dots, a^v}$,若使用a来对样本集D进行划分，会产生V的分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$ a^v$的样本，记为$ D^v$. 
+ 给每个分支赋予权重$ \frac{|D^v|}{|D|}$，即样本越多权重越大。
$$ Gain(D, a) = Ent(D) - \sum^{V}_{v = 1}{\frac{|D^v|}{|D|}Ent(D^v)}$$

用信息增益来进行决策树的划分属性选择。

##### 剪枝处理
剪枝是为了应对决策树的过拟合问题的主要手段。分为：
+ 预剪枝：在增加属性结点的前后分别计算验证集上的准确率，若准确率提升则增加该结点，若没有提升则不添加该结点。
+ 后剪枝：是在生成决策树之后进行的。从低向上对树中的非叶子结点进行考察，从低向上。删除该非叶子结点，在验证集上验证，若结果好于未删除，则删除该结点，反之，不删。

后剪枝通常比预剪枝决策树保留了更多的分支，欠拟合风险小，泛化性能由于预剪枝决策树。但是训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。
